{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search\n",
    "\n",
    "Simple implementation of random search displaying a progress bar for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules and creating Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer, HashingVectorizer\n",
    "\n",
    "# classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "import codecs\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class by Mathias Mueller\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Reads training data and trains a classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=\"model.pkl\", data=None, verbose=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        self._data = data\n",
    "        self._verbose = verbose\n",
    "\n",
    "        # outcomes\n",
    "        self.classes = []\n",
    "        self.num_classes = 0\n",
    "        self.train_X = None\n",
    "        self.train_y = None\n",
    "        self.vectorizer = None\n",
    "        self.classifier = None\n",
    "        self.pipeline = None\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Preprocesses data, fits a model, and finally saves the model to a file.\n",
    "        \"\"\"\n",
    "        self._preprocess()\n",
    "        self._build_pipeline()\n",
    "        self._fit()\n",
    "\n",
    "    def _preprocess(self):\n",
    "        \"\"\"\n",
    "        Reads lines from the raw training data.\n",
    "        \"\"\"\n",
    "        d = defaultdict(list)\n",
    "\n",
    "        if self._data:\n",
    "            data = codecs.open(self._data, \"r\", \"UTF-8\")\n",
    "        else:\n",
    "            logging.warning(\"--data not found, assuming input from STDIN\")\n",
    "            data = sys.stdin\n",
    "\n",
    "        reader = csv.DictReader(data, delimiter=\",\", quotechar='\"')\n",
    "\n",
    "        for row in reader:\n",
    "            X, y = row['Text'], row['Label']\n",
    "            d[y].append(X)\n",
    "\n",
    "        logging.debug(\"Examples per class:\")\n",
    "        for k, v in d.items():\n",
    "            logging.debug(\"%s %d\" % (k, len(v)))\n",
    "        logging.debug(\"Total training examples: %d\\n\" %\n",
    "                      sum([len(v) for v in d.values()]))\n",
    "\n",
    "        self.classes = d.keys()\n",
    "        self.classes = sorted(self.classes)\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        l = []\n",
    "        logging.debug(\"Samples from the data:\")\n",
    "        for k, values in d.items():\n",
    "            logging.debug(\"%s\\t%s\" % (values[0], k))\n",
    "            for value in values:\n",
    "                l.append( (value, k) )\n",
    "\n",
    "        # shuffle, just to be sure\n",
    "        random.shuffle(l)\n",
    "        self.train_X, self.train_y = zip(*l)\n",
    "\n",
    "    def _build_pipeline(self):\n",
    "        \"\"\"\n",
    "        Builds an sklearn Pipeline. The pipeline consists of a kind of\n",
    "        vectorizer, followed by a kind of classifier.\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=None)\n",
    "        # self.classifier = KNeighborsClassifier(n_jobs=-1, algorithm='auto', n_neighbors=100)\n",
    "        self.classifier = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', learning_rate='adaptive',\n",
    "                                        early_stopping=True, n_iter_no_change=10, verbose=True)\n",
    "\n",
    "\n",
    "        self.pipeline = Pipeline([\n",
    "            (\"vectorizer\", self.vectorizer),\n",
    "            (\"clf\", self.classifier)\n",
    "        ])\n",
    "\n",
    "        logging.debug(self.vectorizer)\n",
    "        logging.debug(self.classifier)\n",
    "        logging.debug(self.pipeline)\n",
    "\n",
    "    def _fit(self):\n",
    "        \"\"\"\n",
    "        Fits a model for the preprocessed data.\n",
    "        \"\"\"\n",
    "        self.pipeline.fit(self.train_X, self.train_y)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Saves the whole pipeline to a pickled file.\n",
    "        \"\"\"\n",
    "        from sklearn.externals import joblib\n",
    "        joblib.dump(self.pipeline, self._model)\n",
    "        logging.debug(\"Classifier saved to '%s'\" % self._model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions for controlling vectorization and random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'csv/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-feb34f1b8e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmy_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csv/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmy_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_random_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_trainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-faca00a67c6e>\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"UTF-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--data not found, assuming input from STDIN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'csv/train.csv'"
     ]
    }
   ],
   "source": [
    "def do_random_search(trainer, models):\n",
    "    X = vectorize_features(trainer)\n",
    "    y = trainer.train_y\n",
    "    clf = MLPClassifier()\n",
    "\n",
    "    # things to be tested\n",
    "    hidden_layer_size_range = range(20, 210, 10)\n",
    "    activation_values = ['identity', 'logistic', 'tanh', 'relu']\n",
    "    solver_values = ['lbfgs', 'sgd', 'adam']\n",
    "    learning_rate_values = ['constant', 'invscaling', 'adaptive']\n",
    "    learning_rate_init_values = [0.1, 0.01, 0.001, 0.0001]\n",
    "    momentum_range = [n / 10 for n in range(0, 11, 1)] # only used with solver='sgd'\n",
    "    nesterovs_momentum_values = [True, False] # only used with solver='sgd'\n",
    "\n",
    "    # constants\n",
    "    EARLY_STOPPING = [True]\n",
    "    N_ITER_NO_CHANGE = [5]\n",
    "    VALIDATION_FRACTION = [0.1]\n",
    "\n",
    "    param_grid =   {'hidden_layer_sizes':hidden_layer_size_range,\n",
    "                    'activation':activation_values,\n",
    "                    'solver':solver_values,\n",
    "                    'learning_rate':learning_rate_values,\n",
    "                    'learning_rate_init':learning_rate_init_values,\n",
    "                    'momentum':momentum_range,\n",
    "                    'nesterovs_momentum':nesterovs_momentum_values,\n",
    "                    'early_stopping':EARLY_STOPPING,\n",
    "                    'validation_fraction':VALIDATION_FRACTION,\n",
    "                    'n_iter_no_change':N_ITER_NO_CHANGE}\n",
    "\n",
    "    random_search = RandomizedSearchCV( estimator=clf,\n",
    "                                        param_distributions=param_grid,\n",
    "                                        n_iter=models,\n",
    "                                        cv=10,\n",
    "                                        scoring='accuracy',\n",
    "                                        n_jobs=-1, # will use all available cores\n",
    "                                        return_train_score=True)\n",
    "    with ProgressBar():\n",
    "        random_search.fit(X,y)\n",
    "    return random_search\n",
    "\n",
    "\n",
    "def vectorize_features(trainer):\n",
    "    vec = TfidfVectorizer()\n",
    "    return vec.fit_transform(trainer.train_X)\n",
    "\n",
    "\n",
    "my_trainer = Trainer(data='../csv/train.csv')\n",
    "my_trainer._preprocess()\n",
    "random_search = do_random_search(my_trainer, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting results\n",
    "Visualizing as a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(random_search.cv_results_)\n",
    "df = df.sort_values(by=[\"rank_test_score\"])\n",
    "# print(list(df.columns.values))\n",
    "df_relevant = df[['rank_test_score', 'mean_test_score', 'params', 'mean_fit_time']]\n",
    "df_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
